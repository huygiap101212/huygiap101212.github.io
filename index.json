[
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/5.1-coststructure/",
	"title": "Cost Structure",
	"tags": [],
	"description": "",
	"content": "Cost Structure Provisioned concurrency follows a slightly different pricing structure than on-demand Lambda functions. The on-demand pricing follows the below chart, charging users per request and the execution duration (GB-seconds). Pricing for AWS Lambda can be found on the Pricing Page.\nLambda provisioned concurrency has a slightly different pricing structure. While the price for number of requests remains the same, the charge execution duration (GB-seconds) is lower, but there is an additional charge for the provisioned concurrency duration (GB-seconds).\nLeveraging provisioned concurrency can be actually more economical compared to running on-demand Lambda functions once past a certain threshold! The following graph visualizes the price on the y-axis and percentage of the provisioned concurrency being consumed on the x-axis.\nLooking at the graph, observe that if you add provisioned concurrency to a Lambda function and are consuming more than 60 percent of the provisioned concurrency, you are saving on your Lambda costs compared to using on-demand executions. At 100 percent utilization of the provisioned concurrency, the cost savings are near 17 percent.\n"
},
{
	"uri": "http://huygiap101212.github.io/4-eventfiltering/4.1-deploy/",
	"title": "Deploy",
	"tags": [],
	"description": "",
	"content": "Deploy Access the CloudShell In your CloudShell terminal, run the following command curl \u0026#39;https://static.us-east-1.prod.workshops.aws/public/e637a9ca-6695-4c33-abcd-a66a969f1149/static/code/event-filtering.zip\u0026#39; -o event-filtering.zip Extract the files, and move into the correct directory. unzip event-filtering.zip \u0026amp;\u0026amp; cd event-filtering Package and Deploy the SAM application. This deployment will be an interactive menu, the information to give the menu is below. In the CloudShell terminal run the following command: sam deploy --guided Stack Name: event-filtering-stack AWS Region: your current region (i.e. us-west-2, us-east-1) Confirm changes before deploy: n Allow SAM CLI IAM role creation: leave as default Save arguments to configuration file: leave as default SAM configuration file: leave as default SAM configuration environment: leave as default Verify the SAM template deployed successfully in the CloudShell terminal. "
},
{
	"uri": "http://huygiap101212.github.io/2-logtuning/2.1-deploysamplefunction/",
	"title": "Deploy Sample Function",
	"tags": [],
	"description": "",
	"content": "Deploy Sample Function Access the CloudShell In your CloudShell terminal, run the following command curl \u0026#39;https://static.us-east-1.prod.workshops.aws/public/e637a9ca-6695-4c33-abcd-a66a969f1149/static/code/log-tuning.zip\u0026#39; -o log-tuning.zip Extract the files, and move into the correct directory. unzip log-tuning.zip \u0026amp;\u0026amp; cd log-tuning Browse to the AppConfig documentation to locate the current version of the AppConfig extension. Scroll down the page until you get to the Adding the AWS AppConfig Lambda extension section of the document. Copy the ARN link for the region you are performing this lab in. You will need this value in the next step. Build and deploy the application in your CloudShell terminal. sam build \u0026amp;\u0026amp; sam deploy --guided Leave the defaults for all the prompts except for the Parameter AppConfigARN field. Copy the ARN that you noted down earlier if the version number at the end is different than what pops up as a default. Stack Name: leave as default AWS Region: your current region (i.e. us-west-2, us-east-1) Parameter AppConfigARN: PASTE ARN FROM PREVIOUS STEP HERE Parameter AppConfigProfile: leave as default Confirm changes before deploy: leave as default Allow SAM CLI IAM role creation: leave as default Save arguments to configuration file: leave as default SAM configuration file: leave as default SAM configuration environment: leave as default Once you receive a Successfully created message, browse to the Lambda console. In the filter field, type in appconfig to easily locate the newly created function for this lab. Click on the Lambda function to go into the details window for that function. In the area just under Function overview you will see a menu. Click on the Test tab. In the Test Event section, type something like myevent into the Name field.\nPaste this json into the event field (make sure to delete anything previously in there)\n{ \u0026#34;showextensions\u0026#34;: \u0026#34;function1\u0026#34; } Click the Save Changes button to persist this event. Then click the Test button to execute the function and see the results. Once the test is executed expand the Details section of the Execution result. In the Log Output section observe the logging message output by the function, highlighted with red in the image below. "
},
{
	"uri": "http://huygiap101212.github.io/1-powertuning/1.1-deploythelambdapowertuningtool/",
	"title": "Deploy The Lambda Power Tuning Tool",
	"tags": [],
	"description": "",
	"content": "The Lambda Power Tuning Tool is hosted in the Serverless Application Repository (SAR). SAR allows customers to quickly publish and deploy serverless applications. Customers can configure SAR permissions to allow access to specific accounts, accounts within Organizations, or public access for the ability to launch the application.\nLaunch the Lambda Power Tuning Tool Browse to the Serverless Application Repository. In the left-hand pane, make sure Available Applications is selected. In the filter window, type in AWS Lambda Power Tuning, and make sure Show apps that create custom IAM roles or resource policies is checked. Select the aws-lambda-power-tuning application. On the Review, configure and deploy page, leave the defaults for everything. Scroll to the bottom, check the I acknowledge that this app creates custom IAM roles option and click on Deploy Browse to the CloudFormation Console to review the installation of the power tuning tool. This will take several minutes. Once the stack is complete you can move on to the next section. "
},
{
	"uri": "http://huygiap101212.github.io/3-graviton2/3.1-deploythelambdapowertuningtool/",
	"title": "Deploy The Lambda Power Tuning Tool",
	"tags": [],
	"description": "",
	"content": "Deploy The Lambda Power Tuning Tool We will start by creating a Lambda function written in Python, and deployed on the x86 architecture. Then we will convert the function to an Arm-based Graviton2 architecture. We will use the Lambda Power Tuning tool from Module 1 to compare the performance of the two functions running on the different architectures.\nThe Lambda Power Tuning Tool is hosted on the Serverless Application Repository. The Serverless Application Repository (SAR) allows customers to quickly publish and deploy serverless applications. Customers can configure SAR permissions to allow access to specific accounts, accounts within Organizations, or public access for the ability to launch the application.\nIf you already deployed the Lambda Power Tuning Tool from Module 1 of this workshop, you can skip to the next section of this module.\nBrowse to the Serverless Application Repository. In the left-hand pane, make sure Available Applications is selected. In the filter window, type in AWS Lambda Power Tuning, and make sure Show apps that create custom IAM roles or resource policies is checked. Select the aws-lambda-power-tuning application. On the Review, configure and deploy page, leave the defaults for everything. Scroll to the bottom, check the I acknowledge that this app creates custom IAM roles option and click on Deploy Browse to the CloudFormation Console to review the installation of the power tuning tool. This will take several minutes. Once the stack is complete you can move on to the next section. "
},
{
	"uri": "http://huygiap101212.github.io/1-powertuning/",
	"title": "Power Tuning",
	"tags": [],
	"description": "",
	"content": "1. Power Tuning The goal of this workshop is to understand and gain insight into AWS Lambda functions during development. Specifically understanding how to configure resources for a given Lambda function to optimize for cost and performance. The goal is to understand how the execution time will change by increasing or decreasing the memory allocated to the Lambda function.\nThis workshop will be using the Lambda Power Tuning Project by Alex Casalboni. This project is located in the Serverless Application Repository .\nSome details about this tool:\nIt helps you to tune memory allocation to any given Lambda function deployed in your account.\nThe AWS Lambda Power Tuning Tool is a state machine powered by AWS Step Functions that helps you optimize your Lambda functions for cost and/or performance in a data-driven way.\nThe state machine is designed to be easy to deploy and fast to execute. Also, it\u0026rsquo;s language agnostic so you can optimize any Lambda functions in your account.\nYou provide a Lambda function ARN as input and the state machine will invoke that function with multiple power configurations (from 128MB to 3GB). Then it will analyze all the execution logs and provide suggestions on the best power configuration to minimize cost and/or maximize performance.\nContent Deploy The Lambda Power Tuning Tool Deploy Test Lambda Function Analysis Cleanup "
},
{
	"uri": "http://huygiap101212.github.io/",
	"title": "Serverless Optimization",
	"tags": [],
	"description": "",
	"content": "Serverless Optimization (Performance and Cost) Introduction This workshop will run you through several performance and cost optimization scenarios.\nServerless Architecture Best Practices can be found in the Serverless Application Lens from the AWS Well Architected Framework.\nThis workshop has several sections wrapped around common cost reduction scenarios. The modules are all designed to be completely stand alone, so you can complete each section in isolation, however, please make sure to complete the Setup section first.\nThe modules are layed out in order of ease of implementation. There are three main sections of the workshop.\nPower Tuning and Log Tuning are best practices that are often overlooked, and implementation of these strategies do not effect the overall working of your application.\nGraviton2, Even Filtering, Provisioned Concurrency are configuration based approaches that may require some code touches, or a bit of operational overhead.\nWhat you will learn During this workshop you will learn some best practice technique to optimize your serverless workloads to reduce costs and improve performance. The workshop focuses on AWS Lambda however other services are used throughout including:\nAmazon SQS AWS Step Functions AWS AppConfig Content Power Tuning Log Tuning Graviton2 Even Filtering Provisioned Concurrency "
},
{
	"uri": "http://huygiap101212.github.io/4-eventfiltering/4.2-withouteventfiltering/",
	"title": "Without Event Filtering",
	"tags": [],
	"description": "",
	"content": "Without Event Filtering Now we have the SQS queue and Lambda in place it\u0026rsquo;s time to setup the event source mapping so that the Lambda function receive messages from the queue. First we will setup the trigger without event filtering in place.\nCreate the event source mapping Navigate to the Lambda Console and find the function named event-filtering-stack-VehicleDataProcessor which was created with sam deploy. Note there will be a random id at the end of the name. Click into the function and in the overview screen select Add trigger. Now we need to configure the Lambda function to consume messages from the SQS queue created by SAM. In the drop down list select SQS. Configure the following options: SQS queue: event-filtering-stack-VehicleDataQueue Batch size: 1 (change from default of 10) Batch window 0 (default) Click Add. You will then find a new trigger listed in the Configuration tab with the status of Creating. Wait a few minutes for the status to change to Enabled (you will need to use the refresh button). Any messages sent to the SQS queue will now be processed by this Lambda function! Testing the event source mapping We are now going to test the trigger is working correctly by sending 20 messages to the SQS queue, which in turn should trigger the Lambda function. As the batch size is set to 1 there will be 20 invocations.\nIn the CloudShell terminal change into the directory containing the test files. cd ~/event-filtering/testevents In this directory there is a simple Python script named send-messages.py and a json file named input.json. The json file contains an array of 20 test messages to send to the SQS queue. Each event represents data collected from a vehicle.\nLog into the SQS Console and find the queue named event-filtering-stack-VehicleDataQueue. Select the queue and copy the URL. Back in the CloudShell terminal send the test messages to the SQS queue by running the following: sudo pip3 install boto3 python send-messages.py \u0026lt;your-sqs-queue-url\u0026gt; Now go back to the Lambda Console and select the Monitor tab, then click View CloudWatch logs. Click the Search log group button a the top of the screen to view all log messages from the Lambda function. In the search type Records to filter the logs to one message per invocation, only displaying the message json payload. You will find there are 20 events representing 20 invocations of the Lambda function (one for every message we sent to the queue). "
},
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/5.2-costsavingopportunities/",
	"title": "Cost Saving Opportunities",
	"tags": [],
	"description": "",
	"content": "Cost Saving Opportunities Now you know that enabling provisioned concurrency can not only help your function be hyper-ready, but reduce your Lambda costs. The next natural step is to ask - \u0026ldquo;How do you identify when to enable provisioned concurrency?\u0026rdquo;. First and foremost, you should leverage provisioned concurrency if your application is latency sensitive and needs consistent response times. Next, you can also explore if enabling this feature will help reduce your Lambda costs while getting the aforementioned response time benefit. Let\u0026rsquo;s explore when it makes financial sense to enable this feature.\nWhile it is challenging to always accurately predict your Lambda traffic, provisioned concurrency provides cost saving opportunities when you have consistent and/or gradually changing concurrent execution numbers.\nConsider the following graph that depicts the number of concurrent executions per hour.\nAssuming that the number of concurrent Lambda execution is steady during the hour, you could configure 30 provisioned concurrency for the Lambda function, as shown in the below graph. This will allow you to execute 30 concurrent Lambda functions at a reduced cost.\nHowever, you might also be wondering, \u0026ldquo;Can I dynamically scale my provisioned concurrency up and down?\u0026rdquo;. The short answer is yes, but it does come with some caveats. We will cover how you can dynamically assign your provisioned concurrency similar to the below graph in the coming sections.\n"
},
{
	"uri": "http://huygiap101212.github.io/1-powertuning/1.2-deploytestlambafunction/",
	"title": "Deploy Test Lambda Function",
	"tags": [],
	"description": "",
	"content": "Now that you have the power tuning tool installed, the next step is to run an AWS Lambda function against it. In this section of the module, you will be deploying a Lambda function to test the usage of the Power Tuning Tool. This function is memory intensive, using matrices.\nDeploy the function Be sure to deploy this function in the same region as your Lambda Power Tuning Tool\nBrowse to the AWS Lambda Console. Click on the Create Function option on the upper right-hand side of the console. On the Create function page input the following values, leave the rest at their default values and click Create Function. Author from scratch: selected Function Name: lambda-power-tuning-test Runtime: Python 3.8 Architecture: x86_64 In the Lambda Designer pane, click on Layers or alternatively scroll to the bottom of the page, to the Layers pane. Click on the Add Layer button. On the Add layer page input the following values and click the Add button. Layer Source: AWS Layers selected AWS layers: AWSLambda-Python38-SciPy1x selected Version: latest version available selected Locate the Function Code pane and paste the following code into the editor screen. import json import numpy as np from scipy.spatial import ConvexHull def lambda_handler(event, context): ms = 100 print(\u0026#34;Printing from version on 10302020 - size of matrix\u0026#34;, ms,\u0026#34;x\u0026#34;,ms) print(\u0026#34;\\nFilling the matrix with random integers below 100\\n\u0026#34;) matrix_a = np.random.randint(100, size=(ms, ms)) print(matrix_a) print(\u0026#34;random matrix_b =\u0026#34;) matrix_b = np.random.randint(100, size=(ms, ms)) print(matrix_b) print(\u0026#34;matrix_a * matrix_b = \u0026#34;) print(matrix_a.dot(matrix_b)) num_points = 10 print(num_points, \u0026#34;random points:\u0026#34;) points = np.random.rand(num_points, 2) for i, point in enumerate(points): print(i, \u0026#39;-\u0026gt;\u0026#39;, point) hull = ConvexHull(points) print(\u0026#34;The smallest convex set containing all\u0026#34;, num_points, \u0026#34;points has\u0026#34;, len(hull.simplices), \u0026#34;sides,\\nconnecting points:\u0026#34;) for simplex in hull.simplices: print(simplex[0], \u0026#39;\u0026lt;-\u0026gt;\u0026#39;, simplex[1]) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: \u0026#39;\u0026#39;, \u0026#34;isBase64Encoded\u0026#34;: False } Click Deploy to deploy the new code for the function. In the function menu, click the Configuration option. Click on Edit in the General Configuration pane. On the Edit basic settings page input the following values, leave the remaining at their default and click the Save button: Memory: 3008 Timeout: 5 minutes Test the function On the function menu, select the Test tab. Input a name value and click the Test button to invoke a test of the function. Name: testevent After the function has executed. You will see a success message on the screen. Scroll to the top of the Function page to copy the function ARN, you will need this value for the next portion of the lab. "
},
{
	"uri": "http://huygiap101212.github.io/3-graviton2/3.2-deploythebasefunction/",
	"title": "Deploy the base function",
	"tags": [],
	"description": "",
	"content": "Deploy the base function In this section of the module, you will be deploying a computationally expensive Lambda function written in Python 3.9. The function determines the number of primes within the range of 0 - 10 million.\nOpen the Lambda Console.\nClick the Create function button.\nCreate the new function with the following values and then click the Create function button: Author from scratch: selected\nFunction name: lambda-base-function\nRuntime: Python 3.9\nArchitecture: x86_64\nPaste the following Python code into the source code editor. Take a moment to review the code. Notice that you can make the function more or less computationally intensive by modifying the N value on line 23. import json import math import platform import timeit def primes_up_to(n): primes = [] for i in range(2, n+1): is_prime = True sqrt_i = math.isqrt(i) for p in primes: if p \u0026gt; sqrt_i: break if i % p == 0: is_prime = False break if is_prime: primes.append(i) return primes def lambda_handler(event, context): start_time = timeit.default_timer() N = 1000000 primes = primes_up_to(N) stop_time = timeit.default_timer() elapsed_time = stop_time - start_time response = { \u0026#39;machine\u0026#39;: platform.machine(), \u0026#39;elapsed\u0026#39;: elapsed_time, \u0026#39;message\u0026#39;: \u0026#39;There are {} prime numbers \u0026lt;= {}\u0026#39;.format(len(primes), N) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(response) } Navigate to the Configuration tab and General configuration side tab to update the function timeout by clicking on the Edit button. Update the Timeout value on the Edit basic settings page to 3 minutes and click Save. Create a test for the new function by clicking the test button on the code source screen. On the Configure test event screen give the test a name such as test1 and click Save. Now click the Test button and observe the 200 status code. Gather Metrics from the Lambda Power Tuning Tool Copy the Function ARN and save to a text editor or scratch workspace. You will need this value later. Leave this Lambda function tab open in your browser as you move on.\nOpen a new browser tab and navigate to the AWS Step Functions console.\nOpen the Power Tuning State Machine by clicking on the state machine you deployed in the previous section.\nClick the Start Execution button to begin analyzing the function you created earlier. Paste the following JSON into the Execution input field. Take care to update the Lambda ARN with the value you saved in your scratch space from step 1. Click Start execution. { \u0026#34;lambdaARN\u0026#34;: \u0026#34;YOUR LAMBDA ARN HERE\u0026#34;, \u0026#34;powerValues\u0026#34;: [128, 256, 512, 1024, 2048, 3008], \u0026#34;num\u0026#34;: 10, \u0026#34;payload\u0026#34;: \u0026#34;{}\u0026#34;, \u0026#34;parallelInvocation\u0026#34;: true, \u0026#34;strategy\u0026#34;: \u0026#34;cost\u0026#34; } Watch the graph inspector as the Power Tuning Tool executes its analysis. Wait until the analysis of your function is completed, showing Succeeded in the Execution Status field. Navigate to the Execution output tab to copy the visualization URL. Save this URL to your scratch text editor and then paste the URL into a new browser tab to see the results visualized. "
},
{
	"uri": "http://huygiap101212.github.io/2-logtuning/",
	"title": "Log Tuning",
	"tags": [],
	"description": "",
	"content": "2. Log Tuning This lab is based on the example in the aws-samples/aws-lambda-extensions GitHub repository written by Julian Wood. Please go check it out for other awesome samples and demos.\nLogging can have an impact on the overall cost of your solution. Every Lambda function logs stdin, stdout, and stderr to AWS CloudWatch. AWS CloudWatch Logs charges for usage based on three dimensions, the logs ingested by the system (measured in GB), the log storage based on GB and logs analyzed measured in GBs of data scanned. With this understanding we can manipulate the amount of logs captured and the storage retention in order to reduce the cost of your solution. We will walk through these points in this module.\nYou will first deploy a Lambda function with the AppConfig extension. You will then set the logging levels in AppConfig and watch as how the Lambda function changes behaviors on the fly to make sure you are getting the correct logging level visibility. You will then modify the CloudWatch log retention policies so that the logs will expire logs after 7 days.\nContent Deploy Sample Function Modify Logging Behavior Modify Log Group Retention Cleanup "
},
{
	"uri": "http://huygiap101212.github.io/2-logtuning/2.2-modifyloggingbehavior/",
	"title": "Modify Logging Behavior",
	"tags": [],
	"description": "",
	"content": "Modify Logging Behavior In this section we will be using AWS AppConfig to change the behavior of the Lambda function in order to enable more detailed logging. Verbose logging should only be enabled when required, for example, when troubleshooting or in development phases. Log output should be reduced when not required in order to save costs on log storage and ingestion. This lab demonstrates a way to manage log output configuration using AWS AppConfig.\nBrowse to AWS AppConfig. Click on the DemoExtensionApplication tile. Select the Configuration profile tab, and then click on the LoggingLevel tile. In the Versions area, click on the Create button to set up a new configuration version. Change the loglevel from normal to debug and click on the Create hosted configuration version button. You will now notice there is a version 2 in the Logging Level configuration. Let\u0026rsquo;s deploy this to our Lambda function with a new deployment. Click on the Start Deployment button. On the Start deployment page, select the values shown below. Then click on the Start Deployment button. Environment: Production Hosted configuration version: 2 Deployment strategy: AllAtOnce Once the Deployment status is complete, you can browse back to your appconfig function on the Lambda Console. Test your Lambda function again, just as you did in the previous section. Review the log output. If you do not see the new debug log messages, test the Lambda function again, it may take an invocation to capture the changed state of the logging. "
},
{
	"uri": "http://huygiap101212.github.io/1-powertuning/1.3-analysis/",
	"title": "Analysis",
	"tags": [],
	"description": "",
	"content": "Now that you have deployed the power tuning tool and a test Lambda function, it\u0026rsquo;s time to find the best memory size for this function. The Power Tuning Tool will run our test function with various memory sizes to help determine which configuration provides the best cost and performance. In this section we\u0026rsquo;ll walk through the steps to analyze our function\u0026rsquo;s configuration.\nIn a new browser tab, open the Step Function console. In the left-hand pane, make sure State Machines is selected. Select the powerTuningStateMachine from the State Machines list. In the newly opened powerTuningStateMachine page click the Start Execution button. A Start execution window will open. Replace the information in the Input field with the json below. Make sure to replace the value of the lambdaARN field with the ARN you copied in the previous stage. Then click the Start execution button. { \u0026#34;lambdaARN\u0026#34;: \u0026#34;YOUR LAMBDA ARN HERE\u0026#34;, \u0026#34;powerValues\u0026#34;: [128, 256, 512, 1024, 2048, 3008], \u0026#34;num\u0026#34;: 10, \u0026#34;payload\u0026#34;: \u0026#34;{}\u0026#34;, \u0026#34;parallelInvocation\u0026#34;: true, \u0026#34;strategy\u0026#34;: \u0026#34;cost\u0026#34; } Wait for the execution to complete. Once the Execution Status shows Succeeded, click on the Execution input and output menu tab. Copy the URL from the visualization field and paste it into a new browser tab to review the results. This visualization gives you both recommendations and a visual output of the test results from the Power Tuning Tool. From the analysis we can conclude that to optimize the function for the lowest cost a memory configuration of 512MB would be best. To optimize the function for performance a memory configuration of 512MB would be best.\nIf the function was originally configured with 128MB then increasing the memory to 512MB can reduce the cost AND improve the performance. Increasing the memory above 512MB does not reduce the invocation time and therefore only leads to high costs. This shows the importance of optimizing Lambda functions within your environment.\nThis workshop illustrates the cost and performance tradeoff when designing and executing lambda functions. The Lambda Tuning application can be used to gauge the resource requirements and analyze the cost profile. The Power Tuning application will be used in the later Graviton module to analyze a function performance on various compute architectures.\n"
},
{
	"uri": "http://huygiap101212.github.io/3-graviton2/3.3-deploythegraviton2function/",
	"title": "Deploy the Graviton2 Function",
	"tags": [],
	"description": "",
	"content": "Deploy the Graviton2 Function In this section of the module, you\u0026rsquo;ll be modifying the Lambda function from the previous section to be deployed on a Graviton2 Arm-based architecture instead of the x86 architecture.\nDeploy the Graviton2 arm_64 Lambda Function Return to your Lambda function tab or open a new tab in your browser to the main page of the Lambda Console and navigate to the lambda-base-function we created in the last section. Scroll to the bottom of the page to modify the Runtime settings by clicking on the Edit button. Change the runtime architecture to the Graviton2 type by selecting arm64 and then clicking the Save button. Now click the Test button and observe the 200 status code. Gather Metrics from the Lambda Power Tuning Tool Copy the Function ARN you previously saved to your text editor or scratch workspace. Leave this browser tab open.\nOpen the Lambda Power Tuning Tool you used previously. This can be done by going to the AWS Step Functions console. Click the Start Execution button to begin analyzing the function you just modified to use the arm64 architecture. Paste the following JSON into the Execution input field. Take care to update the Lambda ARN with the value you saved in your scratch space from step 1. Click Start execution. { \u0026#34;lambdaARN\u0026#34;: \u0026#34;YOUR LAMBDA ARN HERE\u0026#34;, \u0026#34;powerValues\u0026#34;: [128, 256, 512, 1024, 2048, 3008], \u0026#34;num\u0026#34;: 10, \u0026#34;payload\u0026#34;: \u0026#34;{}\u0026#34;, \u0026#34;parallelInvocation\u0026#34;: true, \u0026#34;strategy\u0026#34;: \u0026#34;cost\u0026#34; } Watch the graph inspector as the Power Tuning Tool executes its analysis. Wait until the analysis of your function is completed, showing Succeeded in the Execution Status field. Navigate to the Execution input and output tab to copy the visualization URL. Paste the URL into a new browser tab to see the results visualized. You should now have two Lambda Power Tuning Visualization tabs open in your browser Leave this browser tab open.\nCompare the performance metrics from each function To compare the results between the two architectures, click the Compare button on the current visualization. Copy the URL from the other open visualization browser tab and paste it into the field. At the following prompt, enter the name x86. Name the current visualization arm64. You can now visually see both the Invocation Time and Cost comparisons between the x86 and arm64 architecture functions. Observe that the arm64 function performs better in both Cost and Invocation Time in this case. "
},
{
	"uri": "http://huygiap101212.github.io/3-graviton2/",
	"title": "Graviton2",
	"tags": [],
	"description": "",
	"content": "3. Graviton2 In September 2021, AWS announced that AWS Lambda Functions would have the option to be powered by the AWS Gravition2 Processor, an Arm-based architecture designed and built by AWS.\nLambda functions powered by AWS Graviton2 processor can achieve up to 34% better price/performance improvement over x86. Graviton2 functions, using an Arm-based processor architecture, are designed to deliver up to 19% better performance at 20% lower cost for a variety of Serverless workloads, such as web and mobile backends, data, and media processing. With lower latency and better performance, functions powered by AWS Graviton2 processors are ideal for powering mission critical Serverless applications.\nPrice/performance improvements will vary by runtime and source code. In this module we will walk through a method for testing sample code with both architectures and complete a price/performance analysis.\nContent Deploy The Lambda Power Tuning Tool Deploy the base function Deploy the Graviton2 Function Cleanup "
},
{
	"uri": "http://huygiap101212.github.io/2-logtuning/2.3-modifyloggroupretention/",
	"title": "Modify Log Group Retention",
	"tags": [],
	"description": "",
	"content": "Modify Log Group Retention In the last section you learned how to manage function configuration to modify the log output level of your code. In this section you are going to reduce the retention period of the logs to help reduce log storage costs.\nWithin your Lambda function from the previous section on the Lambda menu bar, click on the Monitor tab. Then click on the View CloudWatch logs button. This will open a new browser window, at the log group for our AppConfig demo function. Observe the Retention value listed in the details section. Within the Actions dropdown, select the Edit retention settings option. Select 1 week (7 days) from the dropdown in the Expire events after field, and click Save Validate that the Retention time is now set to 1 week within the Log Group Details Page. After the time period has elapsed, those logs will be discarded, saving on storage costs. "
},
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/5.3-staticprovisionedconcurrency/",
	"title": "Static Provisioned Concurrency",
	"tags": [],
	"description": "",
	"content": "Static Provisioned Concurrency Now we will create a Lambda function and configure provisioned concurrency.\nNavigate to the Lambda console and click the orange Create function button. As we are simply configuring the provisioned concurrency, we will use a blueprint to create our function. Select the Use a blueprint and type in hello-world-python to locate our Lambda function. Then click the configure to continue. Name your Lambda function provisioned-concurrency-test. Leave all the remaining values with their default options selected. Click on the Create function.\nOnce you have created your function, you will have to create an alias or publish a version to configure your provisioned concurrency.\nVersions are commonly used to manage the deployment of your function. They enable the quick visibility of previously functioning code as well as the ability to revert back to a previous version if a new deployment is unsuccessful. Aliases are like a pointer to a specific function version, which saves you the trouble of pointing to a new Lambda version anytime a new deployment is made.\nTo publish a new version of your Lambda function, navigate to the Versions tab and click on Publish new version. You can leave the description blank and click on the orange Publish button to publish your first version of the function.\nNow that you have published a version of your Lambda function, you can now configure provisioned concurrency for this version. Navigate to the Configuration tab, select Provisioned Concurrency from the left hand menu, and click on the Edit button. On the Configure provisioned concurrency page, enter 30 for your provisioned concurrency, and click Save button. When you enable Provisioned Concurrency for a function, the Lambda service will initialize the requested number of execution environments so they can be ready to respond to invocations. This avoids the latency in creating a new execution environment on-demand, often referred to as a \u0026ldquo;cold start\u0026rdquo;. We now have 30 execution environments ready and available to take 30 concurrent executions. Any executions above 30 will be handled in the standard on-demand manner.\n"
},
{
	"uri": "http://huygiap101212.github.io/4-eventfiltering/4.3-usingeventfiltering/",
	"title": "Using Event Filtering",
	"tags": [],
	"description": "",
	"content": "Using Event Filtering In this section we will re-create the event source mapping to the SQS queue with a filter to ensure we only process messages where tire_pressure \u0026lt; 32. Messages that do not match the filter criteria will be automatically removed from the queue.\nCreating the event source mapping with a filter Navigate to the Lambda Console and browse to the Lambda function created by the SAM template named event-filtering-stack-VehicleDataProcessor. Edit the function, click the Configuration tab and delete the SQS trigger created in the previous section. Once the existing trigger has deleted (you may need to refresh) click Add trigger to create a new event source mapping. Configure the SQS trigger as before but this time add in a Filter criteria under Additional settings. Use the filter criteria below to filter out messages where tire_pressure is 32 or higher\n{ \u0026#34;body\u0026#34;: { \u0026#34;tire_pressure\u0026#34;: [{\u0026#34;numeric\u0026#34;: [\u0026#34;\u0026lt;\u0026#34;, 32]}] } } Wait for the trigger status to change to enabled before continuing (you may have to use the refresh button). Testing the event filter In the CloudShell terminal run the send-messages.py script again to send the same 20 events to the queue. Take a note of the time you started the test as you will need this to confirm the number of invocations. cd ~/event-filtering/testevents/ sudo pip3 install boto3 python send-messages.py \u0026lt;your-sqs-queue-url\u0026gt; There are 7/20 events that match the filter criteria and should invoke the Lambda function. The other events will be removed from the queue.\nNow go back to the Lambda Console and select the Monitor tab, then click View CloudWatch logs. Click the Search log group button a the top of the screen to view all log messages from the Lambda function. In the search type Records to filter the logs to one message per invocation, only displaying the message json payload. Since you started the latest test you will find there are only 7 events that have been processed by Lambda rather than all 20. "
},
{
	"uri": "http://huygiap101212.github.io/1-powertuning/1.4-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Delete Lambda Power Tuning Tool NOTE: if you are continuing onto the Graviton2 module you do not need to delete the Lambda Power Tuning tool as this will also be used in that module.\nNavigate to the CloudFormation Console page.\nSelect the serverlessrepo-aws-lambda-power-tuning stack and click the Delete button.\nDelete Test Lambda Function Navigate to the Lambda Console.\nSelect Functions on the left-hand menu. Select lambda-power-tuning-test function, select it and click Actions \u0026gt; Delete.\n"
},
{
	"uri": "http://huygiap101212.github.io/2-logtuning/2.4-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Navigate to the AppConfig Console. Select DemoExtensionApplication \u0026gt; Logging Level. Select version 2 and select Delete. (You need to delete this version before deleting the CloudFormation stack as it was created manually). Select Functions on the left-hand menu. Select appconfig-app-Function1-ytv4JHcOiRtw function and click Actions \u0026gt; Delete. "
},
{
	"uri": "http://huygiap101212.github.io/3-graviton2/3.4-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Remove the AWS Lambda Power Tuning Tool To remove the Lambda Power Tuning Tool, navigate to the CloudFormation page.\nSearch for lambda-power.\nSelect the radio button next to the power-tuning stack and then click the Delete button. Remove the Lambda Function To remove the Lambda Function, navigate to the Lambda Console.\nSearch for \u0026rsquo;lambda-base'\nSelect the checkbox next to the lambda-base-function. Then expand the Actions button and then click the Delete option. "
},
{
	"uri": "http://huygiap101212.github.io/4-eventfiltering/4.4-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " In the CloudShell terminal run the below commands to delete the SAM template. cd ~/event-filtering/ sam delete --stack-name event-filtering-stack --no-prompts "
},
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/5.4-dynamicprovisionedconcurrency/",
	"title": "Dynamic Provisioned Concurrency",
	"tags": [],
	"description": "",
	"content": "Dynamic Provisioned Concurrency In the previous section, we configured 30 provisioned concurrency for the sample Lambda function we just created. Now let\u0026rsquo;s step through configuring dynamic provisioned concurrency. We will leverage Application Auto Scaling to setup our dynamic provisioned concurrency. The target tracking scaling option in application auto scaling will increase or decrease the number resources based on a target value for a specific CloudWatch metric. In our case, we will use LambdaProvisionedConcurrencyUtilization as our target metric, and increase or decrease the provisioned concurrency for a Lambda function. The full list of target tracking metrics for all other services can be found in our documentation.\nOpen the Lambda Console and click the Create function button.\nCreate the new function with the following values and then click the Create function button:\nAuthor from scratch: selected\nFunction name: ProvisionedConcurrency\nRuntime: Python 3.9\nArchitecture: x86_64\nIn your CloudShell terminal, run the following command. aws application-autoscaling register-scalable-target \\ --service-namespace lambda \\ --scalable-dimension lambda:function:ProvisionedConcurrency \\ --resource-id function:provisioned-concurrency-test:1 \\ --min-capacity 0 \\ --max-capacity 50 Run the following command to create a configuration file that specifies the target value for LambdaProvisionedConcurrencyUtilization metric to be 0.7. echo \u0026#34;{\\\u0026#34;TargetValue\\\u0026#34;: 0.7, \\\u0026#34;PredefinedMetricSpecification\\\u0026#34;: {\\\u0026#34;PredefinedMetricType\\\u0026#34;: \\\u0026#34;LambdaProvisionedConcurrencyUtilization\\\u0026#34;}}\u0026#34; \u0026gt; config.json Now, use the following command to apply the scaling policy with the new config.json file. aws application-autoscaling put-scaling-policy \\ --service-namespace lambda \\ --scalable-dimension lambda:function:ProvisionedConcurrency \\ --resource-id function:provisioned-concurrency-test:1 \\ --policy-name lambda-target-tracking-scaling-policy --policy-type TargetTrackingScaling \\ --target-tracking-scaling-policy-configuration file://config.json If the command executed successfully, you will see it return a PolicyARN with two alarms. AlarmHigh and AlarmLow.\nThe auto scaling we have just setup is quite simple. We have configured two alarms. The first alarm triggers when the utilization of provisioned concurrency consistently exceeds 70 percent. The second alarm triggers when utilization is consistently less than 63 percent (90 percent of the 70 percent target). Each of the alarms will either allocate more provisioned concurrency or reduce provisioned concurrency to adjust the utilization value\nNavigate to the CloudWatch console page and click on the Alarms and then All alarms in the left navigation bar. It should show two alarms that you have just setup, they will be in a Insufficient data state as the alarm has been just setup. The Conditions columns show the condition it needs to meet to trigger each of the alarms. If you would like to know the specific condition it needs to meet, you can click on the alarm name to view the details.\nThe two scaling policies setup by the application auto scaling service are as follows.\nScaling out: ProvisionedConcurrencyUtilization \u0026gt; 0.7 for 3 datapoints within 3 minutes\nScaling in: ProvisionedConcurrencyUtilization \u0026lt; 0.63 for 15 datapoints within 15 minutes\nAs this is a managed target tracking scaling policy, the above rules can potentially change at any time. It is important to understand the exact rules behind the auto scaling policy you have setup, as this will determine the efficacy of the policy. If you have an extremely dynamic workload where concurrent execution numbers vary a great amount, it is possible that the auto scaling policy will be suboptimal and end up costing more than just executing on-demand Lambda functions. According to the alarm thresholds specified above, it will take at least 3 minutes to scale up, and 15 minutes to scale down.\n"
},
{
	"uri": "http://huygiap101212.github.io/4-eventfiltering/",
	"title": "Even Filtering",
	"tags": [],
	"description": "",
	"content": "4. Even Filtering When an AWS Lambda function is configured with an event source, the Lambda service triggers a Lambda function for each message or record. The exact behavior depends on the choice of event source and the configuration of the event source mapping. The event source mapping defines how the Lambda service handles incoming messages or records from the event source.\nAt re:Invent 2021, AWS announced the ability to filter messages before the invocation of a Lambda function. Filtering is supported for the following event sources:\nAmazon DynamoDB Amazon Kinesis Data Streams Amazon MQ Amazon Managed Streaming for Apache Kafka (Amazon MSK) Self-managed Apache Kafka Amazon Simple Queue Service (Amazon SQS) Event filtering helps reduce requests made to your Lambda functions, may simplify code, and can reduce overall cost. In this lab we will explore event filtering on a Lambda function subscribed to a an SQS queue as shown in the diagram below.\nWe will use the same example of vehicle IoT data from the event filtering launch blog. We are only interested in processing SQS records where the tire_pressure is less than 32 PSI which may indicate a problem with the tire.\nContent Deploy Without Event Filtering Using Event Filtering Cleanup "
},
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/5.5-conclusionandcleanup/",
	"title": "Conclusion and Cleanup",
	"tags": [],
	"description": "",
	"content": "Provisioned concurrency can not only be used to reduce responses times of your Lambda function, but it can also be leveraged to reduce your Lambda costs. Provisioned concurrency can also be used in conjunction with Compute Savings Plans for additional savings as well. However as under-utilized provisioned concurrency can cost more than on-demand Lambda functions, we advise you to closely monitor Lambda metrics to confirm that you are indeed saving costs by leveraging provisioned concurrency.\nClean up Navigate to the Lambda Console\nSelect two functions, then using the Actions drop down, Delete the function.\nNext navigate to the CloudWatch console .\nNavigate into the Alarms dashboard. Type TargetTracking into the search bar to identify the alarms we created in the last section.\nSelect both of the TargetTracking alarms, and then using the Actions drop down, Delete the alarms.\n"
},
{
	"uri": "http://huygiap101212.github.io/5-provisionedconcurrency/",
	"title": "Provisioned Concurrency",
	"tags": [],
	"description": "",
	"content": "5. Provisioned Concurrency Provisioned concurrency is a feature that keeps functions initialized and hyper-ready to respond to requests within double-digit milliseconds. This is ideal for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, and synchronous APIs that require immediate responses. In this section of the workshop, you will enable provisioned concurrency for one of the Lambda functions that you have created. We show how this feature can not only help your function be hyper-ready, but also reduce your Lambda function costs.\nContent Cost Structure Cost Saving Opportunities Static Provisioned Concurrency Dynamic Provisioned Concurrency Conclusion and Cleanup "
},
{
	"uri": "http://huygiap101212.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://huygiap101212.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]